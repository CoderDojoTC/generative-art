{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Generative Art \u00b6 This site describes the field of generative art: the process of converting narrative text into images. Sample Content What is generative art? Examples of generative art Products that create images from descriptive text Prompt design Concepts Bias Activities Glossary References Contacts","title":"Home"},{"location":"#generative-art","text":"This site describes the field of generative art: the process of converting narrative text into images. Sample Content What is generative art? Examples of generative art Products that create images from descriptive text Prompt design Concepts Bias Activities Glossary References Contacts","title":"Generative Art"},{"location":"01-intro/","text":"Generative Art \u00b6 This site describes the field of generative art: the process of converting narrative text into images. Sample Content What is generative art? Examples of generative art Products that create images from descriptive text Prompt design Concepts Bias Activities Glossary Contacts Source: Types of Cyber Security Roles: Job Growth and Career Paths","title":"Introduction"},{"location":"01-intro/#generative-art","text":"This site describes the field of generative art: the process of converting narrative text into images. Sample Content What is generative art? Examples of generative art Products that create images from descriptive text Prompt design Concepts Bias Activities Glossary Contacts Source: Types of Cyber Security Roles: Job Growth and Career Paths","title":"Generative Art"},{"location":"02-products/","text":"Generative Art Products \u00b6 Dall-E 2 \u00b6 Dall-E 2 from OpenAI is the product that has made generative art a key milestone in technology. It is based on the 175B paramerter GPT-3 model. OpenAI DALL-E 2 MidJourney (MJ) \u00b6 MidJourney is a mature commercial text-to-image system that has many command-line options for changing the size of the images generated. MidJourney.com MidJourny uses a Discord system where the user types /imagnine to a bot followed by a prompt. Stable Diffusion \u00b6 Stable Diffusion is a state-of-the-art text-to-image model that generates images from text. HuggingFace Stable Diffusion Typical wait times are over five minutes. Stable Diffusion is the brainchild of Emad Mostaque, a London-based former hedge fund manager whose aim is to bring novel applications of deep learning to the masses through his company, Stability AI. Craiyon \u00b6 An ad-supported text-to-image generation tool with rendering times around one minute. Craiyon.com Craiyon does well on abstract art, but does not render faces well.","title":"Products"},{"location":"02-products/#generative-art-products","text":"","title":"Generative Art Products"},{"location":"02-products/#dall-e-2","text":"Dall-E 2 from OpenAI is the product that has made generative art a key milestone in technology. It is based on the 175B paramerter GPT-3 model. OpenAI DALL-E 2","title":"Dall-E 2"},{"location":"02-products/#midjourney-mj","text":"MidJourney is a mature commercial text-to-image system that has many command-line options for changing the size of the images generated. MidJourney.com MidJourny uses a Discord system where the user types /imagnine to a bot followed by a prompt.","title":"MidJourney (MJ)"},{"location":"02-products/#stable-diffusion","text":"Stable Diffusion is a state-of-the-art text-to-image model that generates images from text. HuggingFace Stable Diffusion Typical wait times are over five minutes. Stable Diffusion is the brainchild of Emad Mostaque, a London-based former hedge fund manager whose aim is to bring novel applications of deep learning to the masses through his company, Stability AI.","title":"Stable Diffusion"},{"location":"02-products/#craiyon","text":"An ad-supported text-to-image generation tool with rendering times around one minute. Craiyon.com Craiyon does well on abstract art, but does not render faces well.","title":"Craiyon"},{"location":"03-concepts/","text":"Generative Art Concepts \u00b6 Prompt Engineering \u00b6 Zero-Shot Learning \u00b6 Styles \u00b6 Style Transfer \u00b6 Challenges \u00b6 Faces \u00b6 Hands \u00b6","title":"Concepts"},{"location":"03-concepts/#generative-art-concepts","text":"","title":"Generative Art Concepts"},{"location":"03-concepts/#prompt-engineering","text":"","title":"Prompt Engineering"},{"location":"03-concepts/#zero-shot-learning","text":"","title":"Zero-Shot Learning"},{"location":"03-concepts/#styles","text":"","title":"Styles"},{"location":"03-concepts/#style-transfer","text":"","title":"Style Transfer"},{"location":"03-concepts/#challenges","text":"","title":"Challenges"},{"location":"03-concepts/#faces","text":"","title":"Faces"},{"location":"03-concepts/#hands","text":"","title":"Hands"},{"location":"04-prompts/","text":"Prompt Design \u00b6 Prompt design is the art and science of creating a short descriptive text that is given to a language model. Some systems such as OpenAI's GPT-3 also allow you to give a sequence of examples of what you would like returned. In the future, Example with GPT-3 \u00b6 Prompt: 1 2 3 4 5 6 Create an outline for a class on prompt design using large language models such as GPT - 3 . 1 . What is prompt engineering 2 . Designing prompts 3 . Prompt length 4 . Results: 1 2 3 4 5 Prompt type 5. Prompt structure 6. Prompt content 7. Generating prompts with GPT-3 8. Evaluating prompts Prompts in Text to Image \u00b6 In","title":"Prompt Design"},{"location":"04-prompts/#prompt-design","text":"Prompt design is the art and science of creating a short descriptive text that is given to a language model. Some systems such as OpenAI's GPT-3 also allow you to give a sequence of examples of what you would like returned. In the future,","title":"Prompt Design"},{"location":"04-prompts/#example-with-gpt-3","text":"Prompt: 1 2 3 4 5 6 Create an outline for a class on prompt design using large language models such as GPT - 3 . 1 . What is prompt engineering 2 . Designing prompts 3 . Prompt length 4 . Results: 1 2 3 4 5 Prompt type 5. Prompt structure 6. Prompt content 7. Generating prompts with GPT-3 8. Evaluating prompts","title":"Example with GPT-3"},{"location":"04-prompts/#prompts-in-text-to-image","text":"In","title":"Prompts in Text to Image"},{"location":"05-bias/","text":"Bias in Generative Art \u00b6 If you take the prompt, \"A picture of a CIO\", we might get mostly males. This is because many photos that are tagged as \"CEO\" include photos of males. To minimize this bias, generative art programs can detect when occupations are added to the prompt. They can then place gender adjectives before the occupation such as \"male\" or \"female\". Many new generative products do not warn you or correct this problem. Reducing Bias and Improving Safety in DALL\u00b7E 2","title":"Bias"},{"location":"05-bias/#bias-in-generative-art","text":"If you take the prompt, \"A picture of a CIO\", we might get mostly males. This is because many photos that are tagged as \"CEO\" include photos of males. To minimize this bias, generative art programs can detect when occupations are added to the prompt. They can then place gender adjectives before the occupation such as \"male\" or \"female\". Many new generative products do not warn you or correct this problem. Reducing Bias and Improving Safety in DALL\u00b7E 2","title":"Bias in Generative Art"},{"location":"06-fine-tuning/","text":"Fine Tuning a Text-to-Image Model \u00b6 Intro \u00b6 Fine-tuning is the process of adding new layers to an existing model to get it to perform better on a give task. Steps \u00b6 Find an existing model HuggingFace Text-to-Image Models Download the model to your local computer Create a set of images you want to use to fine-tune the model Add new layers to the network and train their weights using these images Waifu Diffusion \u00b6 Waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. The image dataset for fine tuning was 56k images chosesen from the Danbooru image database. Waifu-diffusion Sample Code \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import gradio as gr import torch from torch import autocast from diffusers import StableDiffusionPipeline # our new model ID model_id = \"hakurei/waifu-diffusion\" # use this only if you have a GPU from NVIDIA device = \"cuda\" # we create a new pipeline with our model id using a 16-bit floating point representation pipe = StableDiffusionPipeline . from_pretrained ( model_id , torch_dtype = torch . float16 , revision = 'fp16' ) pipe = pipe . to ( device ) block = gr . Blocks ( css = \".container { max-width: 800px; margin: auto; }\" ) num_samples = 2 def infer ( prompt ): with autocast ( \"cuda\" ): images = pipe ([ prompt ] * num_samples , guidance_scale = 7.5 )[ \"sample\" ] return images with block as demo : gr . Markdown ( \"<h1><center>Waifu Diffusion</center></h1>\" ) gr . Markdown ( \"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\" ) with gr . Group (): with gr . Box (): with gr . Row () . style ( mobile_collapse = False , equal_height = True ): text = gr . Textbox ( label = \"Enter your prompt\" , show_label = False , max_lines = 1 ) . style ( border = ( True , False , True , True ), rounded = ( True , False , False , True ), container = False , ) btn = gr . Button ( \"Run\" ) . style ( margin = False , rounded = ( False , True , True , False ), ) gallery = gr . Gallery ( label = \"Generated images\" , show_label = False ) . style ( grid = [ 2 ], height = \"auto\" ) text . submit ( infer , inputs = [ text ], outputs = gallery ) btn . click ( infer , inputs = [ text ], outputs = gallery ) gr . Markdown ( \"\"\"___ <p style='text-align: center'> Created by https://huggingface.co/hakurei <br/> </p>\"\"\" ) demo . launch ( debug = True )","title":"Fine Tuning"},{"location":"06-fine-tuning/#fine-tuning-a-text-to-image-model","text":"","title":"Fine Tuning a Text-to-Image Model"},{"location":"06-fine-tuning/#intro","text":"Fine-tuning is the process of adding new layers to an existing model to get it to perform better on a give task.","title":"Intro"},{"location":"06-fine-tuning/#steps","text":"Find an existing model HuggingFace Text-to-Image Models Download the model to your local computer Create a set of images you want to use to fine-tune the model Add new layers to the network and train their weights using these images","title":"Steps"},{"location":"06-fine-tuning/#waifu-diffusion","text":"Waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning. The image dataset for fine tuning was 56k images chosesen from the Danbooru image database. Waifu-diffusion","title":"Waifu Diffusion"},{"location":"06-fine-tuning/#sample-code","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 import gradio as gr import torch from torch import autocast from diffusers import StableDiffusionPipeline # our new model ID model_id = \"hakurei/waifu-diffusion\" # use this only if you have a GPU from NVIDIA device = \"cuda\" # we create a new pipeline with our model id using a 16-bit floating point representation pipe = StableDiffusionPipeline . from_pretrained ( model_id , torch_dtype = torch . float16 , revision = 'fp16' ) pipe = pipe . to ( device ) block = gr . Blocks ( css = \".container { max-width: 800px; margin: auto; }\" ) num_samples = 2 def infer ( prompt ): with autocast ( \"cuda\" ): images = pipe ([ prompt ] * num_samples , guidance_scale = 7.5 )[ \"sample\" ] return images with block as demo : gr . Markdown ( \"<h1><center>Waifu Diffusion</center></h1>\" ) gr . Markdown ( \"waifu-diffusion is a latent text-to-image diffusion model that has been conditioned on high-quality anime images through fine-tuning.\" ) with gr . Group (): with gr . Box (): with gr . Row () . style ( mobile_collapse = False , equal_height = True ): text = gr . Textbox ( label = \"Enter your prompt\" , show_label = False , max_lines = 1 ) . style ( border = ( True , False , True , True ), rounded = ( True , False , False , True ), container = False , ) btn = gr . Button ( \"Run\" ) . style ( margin = False , rounded = ( False , True , True , False ), ) gallery = gr . Gallery ( label = \"Generated images\" , show_label = False ) . style ( grid = [ 2 ], height = \"auto\" ) text . submit ( infer , inputs = [ text ], outputs = gallery ) btn . click ( infer , inputs = [ text ], outputs = gallery ) gr . Markdown ( \"\"\"___ <p style='text-align: center'> Created by https://huggingface.co/hakurei <br/> </p>\"\"\" ) demo . launch ( debug = True )","title":"Sample Code"},{"location":"08-whats-next/","text":"What's Next for Generative Art \u00b6 Image resolution \u00b6 Quality \u00b6 Customizability \u00b6 Video \u00b6","title":"Whats Next"},{"location":"08-whats-next/#whats-next-for-generative-art","text":"","title":"What's Next for Generative Art"},{"location":"08-whats-next/#image-resolution","text":"","title":"Image resolution"},{"location":"08-whats-next/#quality","text":"","title":"Quality"},{"location":"08-whats-next/#customizability","text":"","title":"Customizability"},{"location":"08-whats-next/#video","text":"","title":"Video"},{"location":"contacts/","text":"Generative Art Contacts \u00b6 General Code Savvy Contact \u00b6 kidscode@codesavvy.org Contact for CoderDojo Twin Cities \u00b6 hello@coderdojotc.org Specific questions on this repository \u00b6 Dan McCreary","title":"Contacts"},{"location":"contacts/#generative-art-contacts","text":"","title":"Generative Art Contacts"},{"location":"contacts/#general-code-savvy-contact","text":"kidscode@codesavvy.org","title":"General Code Savvy Contact"},{"location":"contacts/#contact-for-coderdojo-twin-cities","text":"hello@coderdojotc.org","title":"Contact for CoderDojo Twin Cities"},{"location":"contacts/#specific-questions-on-this-repository","text":"Dan McCreary","title":"Specific questions on this repository"},{"location":"glossary/","text":"Glossary of Terms for Text to Image Project \u00b6 API Key \u00b6 A key that is assigned to a developer to use a specific programming interface. The key is used to associate billing with the service. Many commercial text-to-image systems require the user to register with a credit card to get an API Key. Images generated are billed to a credit card each month. BERT \u00b6 Bidirectional Encoder Representations from Transformers (BERT) is a transformer -based machine learning technique for [natural language processing] (NLP) pre-training developed by Google. See also: Wikipedia on BERT Content \u00b6 The information is conveyed by an image. For example, the mood, the message, or the story. Deep Neural Network \u00b6 A type of neural network with multiple layers between the input and output layers. See also: Wikipedia on DNN Domain \u00b6 The subject matter or context of an image. For example, a landscape, a portrait, a still life, or an abstract image. Embedding \u00b6 A vector of scalar numbers used to map a word or phrase into the mathematical structure for comparison using distance measurements. Fine Tuning \u00b6 The process of adding a small additional output layer to an existing model to give specific results. For example, diagrams that need to conform to Optum 2022 color themes would need to fine-tune a basic model. Gender Bias \u00b6 The tendency of machine learning models to favor one gender or another for specific situations. The most common of these is occupations or roles. For example, the BERT large language model generates an 83% chance that a \"nurse\" will be female, but only a See also: Showing Bias in BERT Generative Adversarial Network \u00b6 A neural network architecture used for image generation that uses dueling neural networks to genrate and judge the quality of images. Also known as: GAN Perceptual loss \u00b6 A loss function used in image generation that measures the difference between two images in a way that is perceptually meaningful to humans. Generative Model \u00b6 A machine learning model used to generate images from text descriptions. HuggingFace \u00b6 The largest collection of machine learning models and tools to fine-tune these models. As of August 2022, HuggingFace has 28 image to text models. See HuggingFace Text to Image Models Image synthesis \u00b6 The process of creating a new image from a set of input images. Multi-model Models \u00b6 Models that convert one medium into another medium. For example text-to-image, image-to-text, speech-to-text, text-to-speech etc. Many of the models can be fine-tuned to generate other representations such as text-to-line-art or text-to-architecture-diagram. Texture synthesis \u00b6 The process of creating a texture from a set of input textures Neural Network \u00b6 A machine learning model composed of a large number of interconnected processing nodes, or neurons, that can learn to recognize patterns of input data. Text-to-image models almost all use deep neural networks that contain millions or billions of parameters. Max Tokens \u00b6 The maximum number of tokens used by a prompt. After this limit is reached the text is ignored. The max-tokens for standard language models such as BERT are usually 512 tokens. Larger models such as GPT-3 are 2K or 4K tokens. Style \u00b6 The style used to render an image. For example, a photo, a cartoon, a wireframe, a sketch, line art, a painter or a painting. Prompts that desire a specific style can add the suffix such as \"in the style of...\" Style Transfer \u00b6 The process of creating an image in the style of another style, artist, image or type of image. For example, \"A cat in a field in the style of Vincent Van Goh\" Stable Diffusion \u00b6 The most popular text-to-image model on HuggingFace. See HuggingFace Training Bias \u00b6 The training data used to train a machine learning model. This data can influence the results of the model. Prompt \u00b6 The descriptive text that is given to generate an image. Text to Image \u00b6 The process of generating images from a text description using large langauge models. Tokenization \u00b6 The way that descriptive text is converted into a sequence of numeric values. Small common words are usually a single token. Uncommon multi-syllable words are often many tokens.","title":"Glossary"},{"location":"glossary/#glossary-of-terms-for-text-to-image-project","text":"","title":"Glossary of Terms for Text to Image Project"},{"location":"glossary/#api-key","text":"A key that is assigned to a developer to use a specific programming interface. The key is used to associate billing with the service. Many commercial text-to-image systems require the user to register with a credit card to get an API Key. Images generated are billed to a credit card each month.","title":"API Key"},{"location":"glossary/#bert","text":"Bidirectional Encoder Representations from Transformers (BERT) is a transformer -based machine learning technique for [natural language processing] (NLP) pre-training developed by Google. See also: Wikipedia on BERT","title":"BERT"},{"location":"glossary/#content","text":"The information is conveyed by an image. For example, the mood, the message, or the story.","title":"Content"},{"location":"glossary/#deep-neural-network","text":"A type of neural network with multiple layers between the input and output layers. See also: Wikipedia on DNN","title":"Deep Neural Network"},{"location":"glossary/#domain","text":"The subject matter or context of an image. For example, a landscape, a portrait, a still life, or an abstract image.","title":"Domain"},{"location":"glossary/#embedding","text":"A vector of scalar numbers used to map a word or phrase into the mathematical structure for comparison using distance measurements.","title":"Embedding"},{"location":"glossary/#fine-tuning","text":"The process of adding a small additional output layer to an existing model to give specific results. For example, diagrams that need to conform to Optum 2022 color themes would need to fine-tune a basic model.","title":"Fine Tuning"},{"location":"glossary/#gender-bias","text":"The tendency of machine learning models to favor one gender or another for specific situations. The most common of these is occupations or roles. For example, the BERT large language model generates an 83% chance that a \"nurse\" will be female, but only a See also: Showing Bias in BERT","title":"Gender Bias"},{"location":"glossary/#generative-adversarial-network","text":"A neural network architecture used for image generation that uses dueling neural networks to genrate and judge the quality of images. Also known as: GAN","title":"Generative Adversarial Network"},{"location":"glossary/#perceptual-loss","text":"A loss function used in image generation that measures the difference between two images in a way that is perceptually meaningful to humans.","title":"Perceptual loss"},{"location":"glossary/#generative-model","text":"A machine learning model used to generate images from text descriptions.","title":"Generative Model"},{"location":"glossary/#huggingface","text":"The largest collection of machine learning models and tools to fine-tune these models. As of August 2022, HuggingFace has 28 image to text models. See HuggingFace Text to Image Models","title":"HuggingFace"},{"location":"glossary/#image-synthesis","text":"The process of creating a new image from a set of input images.","title":"Image synthesis"},{"location":"glossary/#multi-model-models","text":"Models that convert one medium into another medium. For example text-to-image, image-to-text, speech-to-text, text-to-speech etc. Many of the models can be fine-tuned to generate other representations such as text-to-line-art or text-to-architecture-diagram.","title":"Multi-model Models"},{"location":"glossary/#texture-synthesis","text":"The process of creating a texture from a set of input textures","title":"Texture synthesis"},{"location":"glossary/#neural-network","text":"A machine learning model composed of a large number of interconnected processing nodes, or neurons, that can learn to recognize patterns of input data. Text-to-image models almost all use deep neural networks that contain millions or billions of parameters.","title":"Neural Network"},{"location":"glossary/#max-tokens","text":"The maximum number of tokens used by a prompt. After this limit is reached the text is ignored. The max-tokens for standard language models such as BERT are usually 512 tokens. Larger models such as GPT-3 are 2K or 4K tokens.","title":"Max Tokens"},{"location":"glossary/#style","text":"The style used to render an image. For example, a photo, a cartoon, a wireframe, a sketch, line art, a painter or a painting. Prompts that desire a specific style can add the suffix such as \"in the style of...\"","title":"Style"},{"location":"glossary/#style-transfer","text":"The process of creating an image in the style of another style, artist, image or type of image. For example, \"A cat in a field in the style of Vincent Van Goh\"","title":"Style Transfer"},{"location":"glossary/#stable-diffusion","text":"The most popular text-to-image model on HuggingFace. See HuggingFace","title":"Stable Diffusion"},{"location":"glossary/#training-bias","text":"The training data used to train a machine learning model. This data can influence the results of the model.","title":"Training Bias"},{"location":"glossary/#prompt","text":"The descriptive text that is given to generate an image.","title":"Prompt"},{"location":"glossary/#text-to-image","text":"The process of generating images from a text description using large langauge models.","title":"Text to Image"},{"location":"glossary/#tokenization","text":"The way that descriptive text is converted into a sequence of numeric values. Small common words are usually a single token. Uncommon multi-syllable words are often many tokens.","title":"Tokenization"},{"location":"references/","text":"Generative Art Resources \u00b6 Wikipedia Page on Generative Art Open AI Dall-E 2 Hierarchical Text-Conditional Image Generation with CLIP Latents Ars Technica: With Stable Diffusion, you may never believe what you see online again Sept 6th, 2022 https://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf - This 2015 paper broke ground in the use of GANs to generate human faces.","title":"References"},{"location":"references/#generative-art-resources","text":"Wikipedia Page on Generative Art Open AI Dall-E 2 Hierarchical Text-Conditional Image Generation with CLIP Latents Ars Technica: With Stable Diffusion, you may never believe what you see online again Sept 6th, 2022 https://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf - This 2015 paper broke ground in the use of GANs to generate human faces.","title":"Generative Art Resources"},{"location":"activities/01-intro/","text":"Introduction to Hands-On Activities \u00b6","title":"Introduction"},{"location":"activities/01-intro/#introduction-to-hands-on-activities","text":"","title":"Introduction to Hands-On Activities"},{"location":"activities/02-dalle2/","text":"Dall-E 2 \u00b6","title":"Dall-E 2"},{"location":"activities/02-dalle2/#dall-e-2","text":"","title":"Dall-E 2"},{"location":"activities/03-midjourney/","text":"midjourney \u00b6","title":"Midjourney"},{"location":"activities/03-midjourney/#midjourney","text":"","title":"midjourney"},{"location":"activities/04-stable-diffusion/","text":"Stable Diffusion Lab \u00b6 This lab is challenging since the render time on the HuggingFace site is around five minutes, so we will only perform a few tests. Girl at Computer \u00b6 Prompt: An African girl having fun at her computer. Result: This prompt shows that Stable Diffusion does well at faces, but falls short when rendering hands.","title":"Stable Diffusion"},{"location":"activities/04-stable-diffusion/#stable-diffusion-lab","text":"This lab is challenging since the render time on the HuggingFace site is around five minutes, so we will only perform a few tests.","title":"Stable Diffusion Lab"},{"location":"activities/04-stable-diffusion/#girl-at-computer","text":"Prompt: An African girl having fun at her computer. Result: This prompt shows that Stable Diffusion does well at faces, but falls short when rendering hands.","title":"Girl at Computer"},{"location":"activities/05-crayion/","text":"Crayion Lab \u00b6 Go to the website http://www.craiyon.com and enter a prompt. Example: A happy African girl at her computer","title":"Crayion"},{"location":"activities/05-crayion/#crayion-lab","text":"Go to the website http://www.craiyon.com and enter a prompt. Example: A happy African girl at her computer","title":"Crayion Lab"},{"location":"examples/artists/","text":"Artist with Distinctive Styles \u00b6 Salvador Dali \u00b6 Mexican \u00b6 Diego Rivera \u00b6 Jose Clemente Orozco \u00b6 David Alfaro Siqueiros \u00b6 Historical \u00b6 Rembrandt \u00b6 Graphic Artists \u00b6 Maxfield Parrish \u00b6 Alphonso Mucha \u00b6 Charles Remy Mackintosh \u00b6 American Women Painters \u00b6 Georgia O'Keeffe \u00b6 Mary Cassatt \u00b6","title":"Artists"},{"location":"examples/artists/#artist-with-distinctive-styles","text":"","title":"Artist with Distinctive Styles"},{"location":"examples/artists/#salvador-dali","text":"","title":"Salvador Dali"},{"location":"examples/artists/#mexican","text":"","title":"Mexican"},{"location":"examples/artists/#diego-rivera","text":"","title":"Diego Rivera"},{"location":"examples/artists/#jose-clemente-orozco","text":"","title":"Jose Clemente Orozco"},{"location":"examples/artists/#david-alfaro-siqueiros","text":"","title":"David Alfaro Siqueiros"},{"location":"examples/artists/#historical","text":"","title":"Historical"},{"location":"examples/artists/#rembrandt","text":"","title":"Rembrandt"},{"location":"examples/artists/#graphic-artists","text":"","title":"Graphic Artists"},{"location":"examples/artists/#maxfield-parrish","text":"","title":"Maxfield Parrish"},{"location":"examples/artists/#alphonso-mucha","text":"","title":"Alphonso Mucha"},{"location":"examples/artists/#charles-remy-mackintosh","text":"","title":"Charles Remy Mackintosh"},{"location":"examples/artists/#american-women-painters","text":"","title":"American Women Painters"},{"location":"examples/artists/#georgia-okeeffe","text":"","title":"Georgia O'Keeffe"},{"location":"examples/artists/#mary-cassatt","text":"","title":"Mary Cassatt"},{"location":"examples/girl-with-laptop/","text":"Girl With Laptop \u00b6 These images are inspired by our Code Savvy mission to represent girls using computers in a positive way. Daybreak \u00b6 A girl lying down with a laptop near a pool in front of pillars with trees, a lake and mountains drawn in the style of Daybreak by Maxfield Parrish. Cave Art \u00b6 Photorealistic \u00b6 A 3D render of a young girl with short curly black hair and dark skin having fun programming at a keyboard on her computer with a white background. Graphic Novel \u00b6","title":"Girl With Laptop"},{"location":"examples/girl-with-laptop/#girl-with-laptop","text":"These images are inspired by our Code Savvy mission to represent girls using computers in a positive way.","title":"Girl With Laptop"},{"location":"examples/girl-with-laptop/#daybreak","text":"A girl lying down with a laptop near a pool in front of pillars with trees, a lake and mountains drawn in the style of Daybreak by Maxfield Parrish.","title":"Daybreak"},{"location":"examples/girl-with-laptop/#cave-art","text":"","title":"Cave Art"},{"location":"examples/girl-with-laptop/#photorealistic","text":"A 3D render of a young girl with short curly black hair and dark skin having fun programming at a keyboard on her computer with a white background.","title":"Photorealistic"},{"location":"examples/girl-with-laptop/#graphic-novel","text":"","title":"Graphic Novel"},{"location":"examples/landscapes/","text":"Landscapes \u00b6 MidJourney Examples \u00b6 [Scene] landscape, painting by Ivan Shishkin, photorealistic, highly detailed, hd, hdr, uhd, unreal engine 5, 8k --ar 3:2 --testp","title":"Landscapes"},{"location":"examples/landscapes/#landscapes","text":"","title":"Landscapes"},{"location":"examples/landscapes/#midjourney-examples","text":"[Scene] landscape, painting by Ivan Shishkin, photorealistic, highly detailed, hd, hdr, uhd, unreal engine 5, 8k --ar 3:2 --testp","title":"MidJourney Examples"}]}